{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":85968,"databundleVersionId":9732229,"sourceType":"competition"}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\n\n# Set image size and batch size\nIMG_SIZE = 624\nBATCH_SIZE = 32  # Moderate batch size\nLEARNING_RATE = 0.0001  # Adjusted learning rate\nDROPOUT_RATE = 0  # Increased dropout for stronger regularization\n\n# Data augmentation for training set (slightly adjusted for diversity)\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    rotation_range=30,  # Increased rotation range\n    zoom_range=0.2,     # Increased zoom\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.2,    # Added shear range\n    validation_split=0.2  # Reserve 20% of the data for validation\n)\n\n# Data generator for validation set (no augmentation, just rescaling)\nval_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Load dataset with directory structure and create train/validation splits\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'\n)\nprint(\"Class Indices:\", train_generator.class_indices)  # Expected output: {'real': 0, 'fake': 1}\n\n\n# Define CNN model with Batch Normalization\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(64, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(128, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(256, (3, 3), activation='relu'),\n    BatchNormalization(),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dropout(DROPOUT_RATE),  # Increased dropout\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Callbacks for early stopping, model saving, and learning rate reduction\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy')\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=15, min_lr=1e-9)\n\n# Train the model with increased epochs\nhistory = model.fit(\n    train_generator,\n    epochs=100,  # Increased epochs\n    validation_data=validation_generator,\n    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n)\n\n# Save the final model\nmodel.save('final_model.keras')\n\n# Evaluate the model on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n\n# Plot learning cur\n\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T12:43:18.726486Z","iopub.execute_input":"2024-10-09T12:43:18.726789Z","iopub.status.idle":"2024-10-09T14:07:02.863523Z","shell.execute_reply.started":"2024-10-09T12:43:18.726757Z","shell.execute_reply":"2024-10-09T14:07:02.862036Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Found 1280 images belonging to 2 classes.\nFound 320 images belonging to 2 classes.\nClass Indices: {'fake_train': 0, 'real_train': 1}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1728477845.129088      94 service.cc:145] XLA service 0x784e24007bd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1728477845.129158      94 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1728477845.129164      94 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n2024-10-09 12:44:23.404476: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[32,32,311,311]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,309,309]{3,2,1,0}, f32[64,32,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\n2024-10-09 12:44:23.599700: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.1953423s\nTrying algorithm eng0{} for conv (f32[32,32,311,311]{3,2,1,0}, u8[0]{0}) custom-call(f32[32,64,309,309]{3,2,1,0}, f32[64,32,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convBackwardInput\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0}} is taking a while...\nI0000 00:00:1728477874.907754      94 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 2s/step - accuracy: 0.5716 - loss: 2.8011 - val_accuracy: 0.5000 - val_loss: 21.1589 - learning_rate: 1.0000e-04\nEpoch 2/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.7123 - loss: 0.5741 - val_accuracy: 0.5000 - val_loss: 43.6741 - learning_rate: 1.0000e-04\nEpoch 3/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - accuracy: 0.7382 - loss: 0.5370 - val_accuracy: 0.5000 - val_loss: 63.0798 - learning_rate: 1.0000e-04\nEpoch 4/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.7621 - loss: 0.5105 - val_accuracy: 0.5000 - val_loss: 79.5977 - learning_rate: 1.0000e-04\nEpoch 5/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.7924 - loss: 0.4353 - val_accuracy: 0.5000 - val_loss: 87.1505 - learning_rate: 1.0000e-04\nEpoch 6/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.8377 - loss: 0.3937 - val_accuracy: 0.5000 - val_loss: 84.8633 - learning_rate: 1.0000e-04\nEpoch 7/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.8430 - loss: 0.3504 - val_accuracy: 0.5000 - val_loss: 69.5616 - learning_rate: 1.0000e-04\nEpoch 8/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.8811 - loss: 0.3022 - val_accuracy: 0.5000 - val_loss: 54.4567 - learning_rate: 1.0000e-04\nEpoch 9/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.8635 - loss: 0.2942 - val_accuracy: 0.5000 - val_loss: 26.5739 - learning_rate: 1.0000e-04\nEpoch 10/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9125 - loss: 0.2388 - val_accuracy: 0.5000 - val_loss: 16.2951 - learning_rate: 1.0000e-04\nEpoch 11/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9225 - loss: 0.1939 - val_accuracy: 0.5156 - val_loss: 7.7525 - learning_rate: 1.0000e-04\nEpoch 12/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9246 - loss: 0.1928 - val_accuracy: 0.5000 - val_loss: 5.6904 - learning_rate: 1.0000e-04\nEpoch 13/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9333 - loss: 0.1693 - val_accuracy: 0.5531 - val_loss: 3.0261 - learning_rate: 1.0000e-04\nEpoch 14/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9380 - loss: 0.1436 - val_accuracy: 0.6094 - val_loss: 2.2366 - learning_rate: 1.0000e-04\nEpoch 15/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9512 - loss: 0.1243 - val_accuracy: 0.7969 - val_loss: 0.6658 - learning_rate: 1.0000e-04\nEpoch 16/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9539 - loss: 0.1220 - val_accuracy: 0.8562 - val_loss: 0.4755 - learning_rate: 1.0000e-04\nEpoch 17/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - accuracy: 0.9709 - loss: 0.0803 - val_accuracy: 0.8750 - val_loss: 0.4092 - learning_rate: 1.0000e-04\nEpoch 18/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9717 - loss: 0.0838 - val_accuracy: 0.9469 - val_loss: 0.1380 - learning_rate: 1.0000e-04\nEpoch 19/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9724 - loss: 0.0732 - val_accuracy: 0.9438 - val_loss: 0.1349 - learning_rate: 1.0000e-04\nEpoch 20/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9840 - loss: 0.0521 - val_accuracy: 0.9812 - val_loss: 0.0507 - learning_rate: 1.0000e-04\nEpoch 21/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9812 - loss: 0.0538 - val_accuracy: 0.8594 - val_loss: 0.4398 - learning_rate: 1.0000e-04\nEpoch 22/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9587 - loss: 0.0863 - val_accuracy: 0.9844 - val_loss: 0.0462 - learning_rate: 1.0000e-04\nEpoch 23/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9679 - loss: 0.0841 - val_accuracy: 0.9531 - val_loss: 0.1527 - learning_rate: 1.0000e-04\nEpoch 24/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9648 - loss: 0.0994 - val_accuracy: 0.8625 - val_loss: 0.4015 - learning_rate: 1.0000e-04\nEpoch 25/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9823 - loss: 0.0512 - val_accuracy: 0.9844 - val_loss: 0.0447 - learning_rate: 1.0000e-04\nEpoch 26/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 2s/step - accuracy: 0.9873 - loss: 0.0439 - val_accuracy: 0.9906 - val_loss: 0.0454 - learning_rate: 1.0000e-04\nEpoch 27/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9820 - loss: 0.0479 - val_accuracy: 0.9719 - val_loss: 0.0639 - learning_rate: 1.0000e-04\nEpoch 28/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9927 - loss: 0.0301 - val_accuracy: 0.9750 - val_loss: 0.0570 - learning_rate: 1.0000e-04\nEpoch 30/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9854 - loss: 0.0392 - val_accuracy: 0.9719 - val_loss: 0.1242 - learning_rate: 1.0000e-04\nEpoch 31/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9728 - loss: 0.0663 - val_accuracy: 0.9937 - val_loss: 0.0228 - learning_rate: 1.0000e-04\nEpoch 32/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.9863 - loss: 0.0445 - val_accuracy: 0.8969 - val_loss: 0.3362 - learning_rate: 1.0000e-04\nEpoch 33/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - accuracy: 0.9673 - loss: 0.1086 - val_accuracy: 0.9594 - val_loss: 0.1093 - learning_rate: 1.0000e-04\nEpoch 34/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.0307 - val_accuracy: 0.9719 - val_loss: 0.0727 - learning_rate: 1.0000e-04\nEpoch 35/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - accuracy: 0.9893 - loss: 0.0405 - val_accuracy: 0.9812 - val_loss: 0.0623 - learning_rate: 1.0000e-04\nEpoch 36/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 2s/step - accuracy: 0.9689 - loss: 0.0882 - val_accuracy: 0.9844 - val_loss: 0.0496 - learning_rate: 1.0000e-04\nEpoch 37/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9850 - loss: 0.0326 - val_accuracy: 0.9406 - val_loss: 0.1495 - learning_rate: 1.0000e-04\nEpoch 38/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9667 - loss: 0.0965 - val_accuracy: 0.9750 - val_loss: 0.0802 - learning_rate: 1.0000e-04\nEpoch 39/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - accuracy: 0.9760 - loss: 0.0490 - val_accuracy: 0.9344 - val_loss: 0.1744 - learning_rate: 1.0000e-04\nEpoch 40/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 2s/step - accuracy: 0.9784 - loss: 0.0616 - val_accuracy: 0.9844 - val_loss: 0.0780 - learning_rate: 1.0000e-04\nEpoch 41/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 2s/step - accuracy: 0.9811 - loss: 0.0470 - val_accuracy: 0.9688 - val_loss: 0.0777 - learning_rate: 1.0000e-04\nEpoch 42/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9884 - loss: 0.0284 - val_accuracy: 0.9906 - val_loss: 0.0528 - learning_rate: 1.0000e-04\nEpoch 43/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9933 - loss: 0.0172 - val_accuracy: 0.9688 - val_loss: 0.1332 - learning_rate: 1.0000e-04\nEpoch 44/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step - accuracy: 0.9872 - loss: 0.0330 - val_accuracy: 0.9594 - val_loss: 0.1380 - learning_rate: 1.0000e-04\nEpoch 45/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9819 - loss: 0.0422 - val_accuracy: 0.9375 - val_loss: 0.1929 - learning_rate: 1.0000e-04\nEpoch 46/100\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 2s/step - accuracy: 0.9877 - loss: 0.0227 - val_accuracy: 0.9937 - val_loss: 0.0499 - learning_rate: 1.0000e-04\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 329ms/step - accuracy: 0.9937 - loss: 0.0212\nValidation Accuracy: 99.37%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Plot learning cur\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m \u001b[43mves\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Plot training & validation accuracy values\u001b[39;00m\n\u001b[1;32m    101\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'ves' is not defined"],"ename":"NameError","evalue":"name 'ves' is not defined","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport re  # Import regex module\n\n# Set image size\nIMG_SIZE = 624\n\n# Load the saved model\nmodel = tf.keras.models.load_model('final_model.keras')\n\n# Define the path to the test images directory\ntest_images_dir = '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/test_images/test_images'\n\n# List all image files in the test directory\nimage_files = os.listdir(test_images_dir)\n\n# Prepare a list to hold the images and their corresponding IDs\nimages = []\nids = []\n\nfor filename in image_files:\n    # Use regex to extract the numeric ID from the filename\n    match = re.search(r'(\\d+)', filename)\n    if match:\n        image_id = int(match.group(1))  # Get the numeric ID\n        ids.append(image_id)\n\n        # Load the image\n        img_path = os.path.join(test_images_dir, filename)\n        img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n        img_array = img_to_array(img)  # Convert image to array\n        img_array = img_array / 255.0  # Rescale pixel values to [0, 1]\n        \n        images.append(img_array)\n\n# Convert the list of images to a NumPy array\nimages_array = np.array(images)\n\n# Make predictions on the test set\npredictions = model.predict(images_array)\n\n# Convert probabilities to class labels (0 for fake, 1 for real)\npredicted_classes = (predictions > 0.5).astype(int).flatten()\n\n# Since 'fake_train' is 0 and 'real_train' is 1, we need to reverse this for submission\n# This will map 'fake' (0) to 1 and 'real' (1) to 0\nmapped_classes = 1 - predicted_classes\n\n# Map predicted classes to the desired output format\nsubmission_df = pd.DataFrame({\n    'ID': ids,\n    'TARGET': mapped_classes\n})\n\n# Save to CSV\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Submission file created successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T14:23:40.249948Z","iopub.execute_input":"2024-10-09T14:23:40.250427Z","iopub.status.idle":"2024-10-09T14:23:57.629511Z","shell.execute_reply.started":"2024-10-09T14:23:40.250378Z","shell.execute_reply":"2024-10-09T14:23:57.628494Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step\nSubmission file created successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Set image size and batch size\nIMG_SIZE = 224\nBATCH_SIZE = 32  # Moderate batch size\nLEARNING_RATE = 0.0001  # Adjusted learning rate\nDROPOUT_RATE = 0.4  # Increased dropout for stronger regularization\n\n# Data augmentation for training set (slightly reduced for faster training)\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    rotation_range=15,  # Further reduced rotation range\n    zoom_range=0.15,    # Further reduced zoom\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    validation_split=0.2  # Reserve 20% of the data for validation\n)\n\n# Data generator for validation set (no augmentation, just rescaling)\nval_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Load dataset with directory structure and create train/validation splits\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'\n)\n\n# Define CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(85, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(DROPOUT_RATE),  # Increased dropout\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Callbacks for early stopping, model saving, and learning rate reduction\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy')\n\n# Reduce learning rate when a metric has stopped improving\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-9)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    epochs=50,\n    validation_data=validation_generator,\n    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n)\n\n# Save the final model\nmodel.save('final_model.keras')\n\n# Evaluate the model on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T11:59:16.542357Z","iopub.execute_input":"2024-10-01T11:59:16.542757Z","iopub.status.idle":"2024-10-01T12:02:57.855724Z","shell.execute_reply.started":"2024-10-01T11:59:16.542719Z","shell.execute_reply":"2024-10-01T12:02:57.854809Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Found 1280 images belonging to 2 classes.\nFound 320 images belonging to 2 classes.\nEpoch 1/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 406ms/step - accuracy: 0.5084 - loss: 0.7167 - val_accuracy: 0.4969 - val_loss: 0.6925 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 404ms/step - accuracy: 0.5298 - loss: 0.6922 - val_accuracy: 0.5531 - val_loss: 0.6904 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 402ms/step - accuracy: 0.5150 - loss: 0.6893 - val_accuracy: 0.6094 - val_loss: 0.6769 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 385ms/step - accuracy: 0.5226 - loss: 0.6895 - val_accuracy: 0.5906 - val_loss: 0.6616 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 417ms/step - accuracy: 0.6214 - loss: 0.6701 - val_accuracy: 0.6313 - val_loss: 0.6674 - learning_rate: 1.0000e-04\nEpoch 6/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 422ms/step - accuracy: 0.5853 - loss: 0.6695 - val_accuracy: 0.6562 - val_loss: 0.6405 - learning_rate: 1.0000e-04\nEpoch 7/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 402ms/step - accuracy: 0.6296 - loss: 0.6526 - val_accuracy: 0.6062 - val_loss: 0.6482 - learning_rate: 1.0000e-04\nEpoch 8/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 391ms/step - accuracy: 0.6553 - loss: 0.6322 - val_accuracy: 0.5938 - val_loss: 0.6567 - learning_rate: 1.0000e-04\nEpoch 9/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 393ms/step - accuracy: 0.6875 - loss: 0.6058 - val_accuracy: 0.5875 - val_loss: 0.7212 - learning_rate: 1.0000e-04\nEpoch 10/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 396ms/step - accuracy: 0.7148 - loss: 0.5856 - val_accuracy: 0.6219 - val_loss: 0.6869 - learning_rate: 5.0000e-05\nEpoch 11/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 390ms/step - accuracy: 0.7283 - loss: 0.5670 - val_accuracy: 0.6438 - val_loss: 0.6174 - learning_rate: 5.0000e-05\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - accuracy: 0.6449 - loss: 0.6516\nValidation Accuracy: 65.62%\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Set image size and batch size\nIMG_SIZE = 224\nBATCH_SIZE = 32  # Moderate batch size\nLEARNING_RATE = 0.0001  # Adjusted learning rate\nDROPOUT_RATE = 0.5  # Increased dropout for regularization\n\n# Data augmentation for training set (slightly reduced for faster training)\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    rotation_range=15,\n    zoom_range=0.15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    validation_split=0.2  # Reserve 20% of the data for validation\n)\n\n# Data generator for validation set (no augmentation, just rescaling)\nval_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Load dataset with directory structure and create train/validation splits\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'\n)\n\nprint(\"Class Indices:\", train_generator.class_indices)  # Expected output: {'real': 0, 'fake': 1}\n\n# Define a more complex CNN model\nmodel = Sequential([\n    Conv2D(64, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),  # Increased filters\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(128, (3, 3), activation='relu'),  # Increased filters\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(256, (3, 3), activation='relu'),  # Further increased filters\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(512, (3, 3), activation='relu'),  # Added another convolutional layer with high filters\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Flatten(),\n    Dense(256, activation='relu'),  # Increased units in dense layer\n    Dropout(DROPOUT_RATE),  # Increased dropout for stronger regularization\n    Dense(128, activation='relu'),  # Added another fully connected layer\n    Dropout(DROPOUT_RATE),\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n\n# Compile the model with the Adam optimizer\nmodel.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Callbacks for early stopping, model saving, and learning rate reduction\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy')\n\n# Reduce learning rate when a metric has stopped improving\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-9)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    epochs=50,\n    validation_data=validation_generator,\n    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n)\n\n# Save the final model\nmodel.save('final_model.keras')\n\n# Evaluate the model on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:42:56.277635Z","iopub.execute_input":"2024-10-01T12:42:56.278349Z","iopub.status.idle":"2024-10-01T12:48:22.542589Z","shell.execute_reply.started":"2024-10-01T12:42:56.278308Z","shell.execute_reply":"2024-10-01T12:48:22.541773Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Found 1280 images belonging to 2 classes.\nFound 320 images belonging to 2 classes.\nClass Indices: {'fake_train': 0, 'real_train': 1}\nEpoch 1/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 438ms/step - accuracy: 0.4943 - loss: 0.6993 - val_accuracy: 0.5250 - val_loss: 0.6927 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 402ms/step - accuracy: 0.5179 - loss: 0.6927 - val_accuracy: 0.5000 - val_loss: 0.6921 - learning_rate: 1.0000e-04\nEpoch 3/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 406ms/step - accuracy: 0.4878 - loss: 0.6936 - val_accuracy: 0.5063 - val_loss: 0.6845 - learning_rate: 1.0000e-04\nEpoch 4/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 405ms/step - accuracy: 0.5403 - loss: 0.6911 - val_accuracy: 0.5000 - val_loss: 0.6751 - learning_rate: 1.0000e-04\nEpoch 5/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 404ms/step - accuracy: 0.5163 - loss: 0.6829 - val_accuracy: 0.5219 - val_loss: 0.6622 - learning_rate: 5.0000e-05\nEpoch 6/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 426ms/step - accuracy: 0.5867 - loss: 0.6698 - val_accuracy: 0.5406 - val_loss: 0.6926 - learning_rate: 5.0000e-05\nEpoch 7/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 433ms/step - accuracy: 0.5583 - loss: 0.6843 - val_accuracy: 0.6125 - val_loss: 0.6379 - learning_rate: 5.0000e-05\nEpoch 8/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 410ms/step - accuracy: 0.6121 - loss: 0.6558 - val_accuracy: 0.5406 - val_loss: 0.7062 - learning_rate: 5.0000e-05\nEpoch 9/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 431ms/step - accuracy: 0.6272 - loss: 0.6452 - val_accuracy: 0.6313 - val_loss: 0.6166 - learning_rate: 5.0000e-05\nEpoch 10/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 408ms/step - accuracy: 0.6373 - loss: 0.6301 - val_accuracy: 0.5719 - val_loss: 0.7428 - learning_rate: 5.0000e-05\nEpoch 11/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 432ms/step - accuracy: 0.6684 - loss: 0.6232 - val_accuracy: 0.6500 - val_loss: 0.6147 - learning_rate: 5.0000e-05\nEpoch 12/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 402ms/step - accuracy: 0.7191 - loss: 0.5700 - val_accuracy: 0.5344 - val_loss: 0.9623 - learning_rate: 5.0000e-05\nEpoch 13/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 408ms/step - accuracy: 0.7539 - loss: 0.5491 - val_accuracy: 0.5312 - val_loss: 0.9454 - learning_rate: 5.0000e-05\nEpoch 14/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 408ms/step - accuracy: 0.6987 - loss: 0.5864 - val_accuracy: 0.6000 - val_loss: 0.7517 - learning_rate: 5.0000e-05\nEpoch 15/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 400ms/step - accuracy: 0.7709 - loss: 0.5313 - val_accuracy: 0.5437 - val_loss: 1.0256 - learning_rate: 2.5000e-05\nEpoch 16/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 412ms/step - accuracy: 0.7746 - loss: 0.5104 - val_accuracy: 0.5281 - val_loss: 1.2659 - learning_rate: 2.5000e-05\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 79ms/step - accuracy: 0.6451 - loss: 0.6195\nValidation Accuracy: 65.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nimport re  # Import regex module\n\n# Set image size\nIMG_SIZE = 224\n\n# Load the saved model\nmodel = tf.keras.models.load_model('final_model.keras')\n\n# Define the path to the test images directory\ntest_images_dir = '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/test_images/test_images'\n\n# List all image files in the test directory\nimage_files = os.listdir(test_images_dir)\n\n# Prepare a list to hold the images and their corresponding IDs\nimages = []\nids = []\n\nfor filename in image_files:\n    # Use regex to extract the numeric ID from the filename\n    match = re.search(r'(\\d+)', filename)\n    if match:\n        image_id = int(match.group(1))  # Get the numeric ID\n        ids.append(image_id)\n\n        # Load the image\n        img_path = os.path.join(test_images_dir, filename)\n        img = load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n        img_array = img_to_array(img)  # Convert image to array\n        img_array = img_array / 255.0  # Rescale pixel values to [0, 1]\n        \n        images.append(img_array)\n\n# Convert the list of images to a NumPy array\nimages_array = np.array(images)\n\n# Make predictions on the test set\npredictions = model.predict(images_array)\n\n# Convert probabilities to class labels (0 for fake, 1 for real)\npredicted_classes = (predictions > 0.5).astype(int).flatten()\n\n# Since 'fake_train' is 0 and 'real_train' is 1, we need to reverse this for submission\n# This will map 'fake' (0) to 1 and 'real' (1) to 0\nmapped_classes = 1 - predicted_classes\n\n# Map predicted classes to the desired output format\nsubmission_df = pd.DataFrame({\n    'ID': ids,\n    'TARGET': mapped_classes\n})\n\n# Save to CSV\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Submission file created successfully!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T13:03:22.472660Z","iopub.execute_input":"2024-10-01T13:03:22.473074Z","iopub.status.idle":"2024-10-01T13:03:31.069288Z","shell.execute_reply.started":"2024-10-01T13:03:22.473035Z","shell.execute_reply":"2024-10-01T13:03:31.068255Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 236ms/step\nSubmission file created successfully!\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\n# Set image size and batch size\nIMG_SIZE = 224\nBATCH_SIZE = 32  # Moderate batch size\nLEARNING_RATE = 0.0001  # Adjusted learning rate\nDROPOUT_RATE = 0.4  # Increased dropout for stronger regularization\n\n# Data augmentation for training set (slightly reduced for faster training)\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    rotation_range=15,  # Further reduced rotation range\n    zoom_range=0.15,    # Further reduced zoom\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    validation_split=0.2  # Reserve 20% of the data for validation\n)\n\n# Data generator for validation set (no augmentation, just rescaling)\nval_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n\n# Load dataset with directory structure and create train/validation splits\ntrain_generator = train_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='training'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/train_images/train_images',\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    subset='validation'\n)\n\n# Check that class indices are correct\nprint(\"Class Indices:\", train_generator.class_indices)  # Expected output: {'real': 0, 'fake': 1}\n\n# Define CNN model\nmodel = Sequential([\n    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(85, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Conv2D(128, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(DROPOUT_RATE),  # Increased dropout\n    Dense(1, activation='sigmoid')  # Binary classification\n])\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Callbacks for early stopping, model saving, and learning rate reduction\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_accuracy')\n\n# Reduce learning rate when a metric has stopped improving\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3, min_lr=1e-9)\n\n# Train the model\nhistory = model.fit(\n    train_generator,\n    epochs=50,\n    validation_data=validation_generator,\n    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n)\n\n# Save the final model\nmodel.save('final_model.keras')\n\n# Evaluate the model on the validation set\nval_loss, val_accuracy = model.evaluate(validation_generator)\nprint(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-01T12:25:16.505232Z","iopub.execute_input":"2024-10-01T12:25:16.505655Z","iopub.status.idle":"2024-10-01T12:25:47.101949Z","shell.execute_reply.started":"2024-10-01T12:25:16.505617Z","shell.execute_reply":"2024-10-01T12:25:47.100314Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found 1280 images belonging to 2 classes.\nFound 320 images belonging to 2 classes.\nClass Indices: {'fake_train': 0, 'real_train': 1}\nEpoch 1/50\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 438ms/step - accuracy: 0.4785 - loss: 0.7090 - val_accuracy: 0.5063 - val_loss: 0.6889 - learning_rate: 1.0000e-04\nEpoch 2/50\n\u001b[1m 3/40\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 435ms/step - accuracy: 0.4479 - loss: 0.6981","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 78\u001b[0m\n\u001b[1;32m     75\u001b[0m reduce_lr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Save the final model\u001b[39;00m\n\u001b[1;32m     86\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n\n# Load your trained model\nmodel = tf.keras.models.load_model('final_model.keras')\n\n# Set image size and batch size for the test set\nIMG_SIZE = 224\nBATCH_SIZE = 32\n\n# Path to the test images\ntest_dir = '/kaggle/input/wec-intelligence-sig-2024-recruitment-task-cv/test_images/test_images/'\n\n# Prepare the test data generator (no augmentation, only rescaling)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\n# Load test images and create a generator\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode=None,\n    shuffle=False\n)\n\n# Make predictions\npredictions = model.predict(test_generator)\n\n# Convert predictions to binary values (0 or 1)\npredicted_classes = (predictions > 0.5).astype(int)\n\n# Get image IDs from the filenames\nimage_ids = test_generator.filenames\n\n# Create a DataFrame for submission\nsubmission_df = pd.DataFrame({\n    'ID': [os.path.splitext(os.path.basename(img))[0] for img in image_ids],\n    'TARGET': predicted_classes.flatten()\n})\n\n# Save to submission.csv\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(submission_df.head())\n","metadata":{},"execution_count":null,"outputs":[]}]}